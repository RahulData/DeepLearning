{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "#Looking at the features we can see that row no.,name will have no relation with a customer with leaving the bank\n",
    "#so we drop them from X which contains the features Indexes from 3 to 12\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "#We store the Dependent value/predicted value in y by storing the 13th index in the variable y\n",
    "y = dataset.iloc[:, 13].values\n",
    "#Printing out the values of X --> Which contains the features\n",
    "#                           y --> Which contains the target variable\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' 42 2 0.0 1 1 1 101348.88]\n",
      " [608 'Spain' 'Female' 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 'France' 'Female' 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 'France' 'Female' 39 1 0.0 2 0 0 93826.63]\n",
      " [850 'Spain' 'Female' 43 2 125510.82 1 1 1 79084.1]]\n"
     ]
    }
   ],
   "source": [
    "# 10 input columns\n",
    "print(X[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# Now we encode the string values in the features to numerical values\n",
    "# The only 2 values are Gender and Region which need to converted into numerical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "#creating label encoder object no. 1 to encode region name(index 1 in features)\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "#encoding region from string to just 3 no.s 0,1,2 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 0 'Female' 42 2 0.0 1 1 1 101348.88]\n",
      " [608 2 'Female' 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 0 'Female' 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 0 'Female' 39 1 0.0 2 0 0 93826.63]\n",
      " [850 2 'Female' 43 2 125510.82 1 1 1 79084.1]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder_X_2 = LabelEncoder()\n",
    "#creating label encoder object no. 2 to encode Gender name(index 2 in features)\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "#encoding Gender from string to just 2 no.s 0,1(male,female) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 0 0 42 2 0.0 1 1 1 101348.88]\n",
      " [608 2 0 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 0 0 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 0 0 39 1 0.0 2 0 0 93826.63]\n",
      " [850 2 0 43 2 125510.82 1 1 1 79084.1]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1'],\n",
       "       ['0', '3'],\n",
       "       ['2', '2'],\n",
       "       ['1', '10'],\n",
       "       ['0', '7']], dtype='<U6')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "exampl = np.array([['Male', 1], ['Female', 3], ['Trans', 2], ['Male',10],['Female',7]])\n",
    "labenc = LabelEncoder()\n",
    "exampl[:, 0] = labenc.fit_transform(exampl[:, 0])\n",
    "exampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  3.],\n",
       "       [ 0.,  0.,  1.,  2.],\n",
       "       [ 0.,  1.,  0., 10.],\n",
       "       [ 1.,  0.,  0.,  7.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(categorical_features = [0])\n",
    "exampl = enc.fit_transform(exampl).toarray()\n",
    "exampl\n",
    "# enc.transform([['Female', 1], ['Male', 4]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now creating Dummy variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 6.1900000e+02 0.0000000e+00 4.2000000e+01\n",
      "  2.0000000e+00 0.0000000e+00 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  1.0134888e+05]\n",
      " [1.0000000e+00 0.0000000e+00 6.0800000e+02 0.0000000e+00 4.1000000e+01\n",
      "  1.0000000e+00 8.3807860e+04 1.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      "  1.1254258e+05]\n",
      " [0.0000000e+00 0.0000000e+00 5.0200000e+02 0.0000000e+00 4.2000000e+01\n",
      "  8.0000000e+00 1.5966080e+05 3.0000000e+00 1.0000000e+00 0.0000000e+00\n",
      "  1.1393157e+05]\n",
      " [0.0000000e+00 0.0000000e+00 6.9900000e+02 0.0000000e+00 3.9000000e+01\n",
      "  1.0000000e+00 0.0000000e+00 2.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  9.3826630e+04]\n",
      " [1.0000000e+00 0.0000000e+00 8.5000000e+02 0.0000000e+00 4.3000000e+01\n",
      "  2.0000000e+00 1.2551082e+05 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      "  7.9084100e+04]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.0000000e+00, 0.0000000e+00, 6.6700000e+02, 0.0000000e+00,\n",
       "         3.4000000e+01, 5.0000000e+00, 0.0000000e+00, 2.0000000e+00,\n",
       "         1.0000000e+00, 0.0000000e+00, 1.6383064e+05],\n",
       "        [0.0000000e+00, 1.0000000e+00, 4.2700000e+02, 1.0000000e+00,\n",
       "         4.2000000e+01, 1.0000000e+00, 7.5681520e+04, 1.0000000e+00,\n",
       "         1.0000000e+00, 1.0000000e+00, 5.7098000e+04],\n",
       "        [0.0000000e+00, 0.0000000e+00, 5.3500000e+02, 0.0000000e+00,\n",
       "         2.9000000e+01, 2.0000000e+00, 1.1236734e+05, 1.0000000e+00,\n",
       "         1.0000000e+00, 0.0000000e+00, 1.8563076e+05],\n",
       "        [1.0000000e+00, 0.0000000e+00, 6.5400000e+02, 1.0000000e+00,\n",
       "         4.0000000e+01, 5.0000000e+00, 1.0568363e+05, 1.0000000e+00,\n",
       "         1.0000000e+00, 0.0000000e+00, 1.7361709e+05],\n",
       "        [1.0000000e+00, 0.0000000e+00, 8.5000000e+02, 0.0000000e+00,\n",
       "         5.7000000e+01, 8.0000000e+00, 1.2677630e+05, 2.0000000e+00,\n",
       "         1.0000000e+00, 1.0000000e+00, 1.3229849e+05]]),\n",
       " array([[0.0000000e+00, 1.0000000e+00, 5.9700000e+02, 0.0000000e+00,\n",
       "         3.5000000e+01, 8.0000000e+00, 1.3110104e+05, 1.0000000e+00,\n",
       "         1.0000000e+00, 1.0000000e+00, 1.9285267e+05],\n",
       "        [0.0000000e+00, 0.0000000e+00, 5.2300000e+02, 0.0000000e+00,\n",
       "         4.0000000e+01, 2.0000000e+00, 1.0296741e+05, 1.0000000e+00,\n",
       "         1.0000000e+00, 0.0000000e+00, 1.2870210e+05]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5,:], X_test[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.74309049, -0.5698444 ,  0.16958176, ...,  0.64259497,\n",
       "        -1.03227043,  1.10643166],\n",
       "       [-0.57369368,  1.75486502, -2.30455945, ...,  0.64259497,\n",
       "         0.9687384 , -0.74866447],\n",
       "       [-0.57369368, -0.5698444 , -1.19119591, ...,  0.64259497,\n",
       "        -1.03227043,  1.48533467],\n",
       "       ...,\n",
       "       [-0.57369368, -0.5698444 ,  0.9015152 , ...,  0.64259497,\n",
       "        -1.03227043,  1.41231994],\n",
       "       [ 1.74309049, -0.5698444 , -0.62420521, ...,  0.64259497,\n",
       "         0.9687384 ,  0.84432121],\n",
       "       [-0.57369368,  1.75486502, -0.28401079, ...,  0.64259497,\n",
       "        -1.03227043,  0.32472465]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57369368,  1.75486502, -0.55204276, -1.09168714, -0.36890377,\n",
       "         1.04473698,  0.8793029 , -0.92159124,  0.64259497,  0.9687384 ,\n",
       "         1.61085707],\n",
       "       [-0.57369368, -0.5698444 , -1.31490297, -1.09168714,  0.10961719,\n",
       "        -1.031415  ,  0.42972196, -0.92159124,  0.64259497, -1.03227043,\n",
       "         0.49587037]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Now let's make the ANN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing out the steps involved in training the ANN with Stochastic Gradient Descent\n",
    "1)Randomly initialize the weights to small numbers close to 0(But not 0)\n",
    "2)Input the 1st observation of your dataset in the Input Layer, each Feature in one Input Node\n",
    "3)Forward-Propagation from Left to Right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights.Propagate the activations until getting the predicted result y.\n",
    "4)Compare the predicted result with the actual result. Measure the generated error.\n",
    "5)Back-Propagation: From Right to Left, Error is back  propagated.Update the weights according to how much they are\n",
    "responsible for the error.The Learning Rate tells us by how much such we update the weights.\n",
    "6)Repeat Steps 1 to 5 and update the weights after each observation(Reinforcement Learning).\n",
    "Or: Repeat Steps 1 to 5 but update the weights only after a batch of observations(Batch Learning)  \n",
    "7)When the whole training set is passed through the ANN.That completes an Epoch. Redo more Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential#For building the Neural Network layer by layer\n",
    "from keras.layers import Dense#To randomly initialize the weights to small numbers close to 0(But not 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "#So there are actually 2 ways of initializing a deep learning model\n",
    "#------1)Defining each layer one by one\n",
    "#------2)Defining a Graph\n",
    "classifier = Sequential()\n",
    "#We did not put any parameter in the Sequential object as we will be defining the Layers manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "#This remains an unanswered question till date that how many nodes of the hidden layer do we actually need\n",
    "# There is no thumb rule but you can set the number of nodes in Hidden Layers as an Average of the number of Nodes in Input and Output Layer Respectively.\n",
    "#Here avg= (11+1)/2==>6 So set Output Dim=6\n",
    "#Init will initialize the Hidden Layer weights uniformly\n",
    "#Activation Function is Rectifier Activation Function\n",
    "#Input dim tells us the number of nodes in the Input Layer.This is done only once and wont be specified in further layers.\n",
    "# classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "# classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "# classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sigmoid activation function is used whenever we need Probabilities of 2 categories or less(Similar to Logistic Regression)\n",
    "#Switch to Softmax when the dependent variable has more than 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahsood\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 3s 359us/step - loss: 0.4828 - acc: 0.7954\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.4278 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.4221 - acc: 0.7984\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4189 - acc: 0.8210\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.4162 - acc: 0.8259\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.4153 - acc: 0.8289\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 0.4138 - acc: 0.8314\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 0.4121 - acc: 0.8319\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 0.4117 - acc: 0.8320\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 3s 352us/step - loss: 0.4103 - acc: 0.8334\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 3s 330us/step - loss: 0.4099 - acc: 0.8334\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.4093 - acc: 0.8337\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4083 - acc: 0.8340\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 3s 357us/step - loss: 0.4078 - acc: 0.8335\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.4068 - acc: 0.8345\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 0.4064 - acc: 0.8359\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 0.4062 - acc: 0.8339\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4057 - acc: 0.8344\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.4059 - acc: 0.8350\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4054 - acc: 0.8350\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 0.4047 - acc: 0.8339\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.4046 - acc: 0.8354\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4045 - acc: 0.8334 1s\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4041 - acc: 0.8332\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.4037 - acc: 0.8344\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.4034 - acc: 0.8356\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4037 - acc: 0.8346\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4033 - acc: 0.8352\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.4035 - acc: 0.8349\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 0.4034 - acc: 0.8342\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.4031 - acc: 0.8346\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.4032 - acc: 0.8339\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.4031 - acc: 0.8349\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4022 - acc: 0.8344\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 0.4025 - acc: 0.8349\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.4023 - acc: 0.8347\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.4020 - acc: 0.8356\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4023 - acc: 0.8362\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.4016 - acc: 0.8360\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.4020 - acc: 0.8341\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4016 - acc: 0.8366\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.4014 - acc: 0.8351\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4013 - acc: 0.8351 0s - loss: 0.3930 -\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4016 - acc: 0.8340\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4017 - acc: 0.8354\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4015 - acc: 0.8337\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4013 - acc: 0.8340\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4011 - acc: 0.8330\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.4013 - acc: 0.8355\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 0.4009 - acc: 0.8349\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 285us/step - loss: 0.4012 - acc: 0.8341\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.4008 - acc: 0.8334 0s - loss: 0.4013 - ac\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4011 - acc: 0.8360\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.4006 - acc: 0.8345\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.4009 - acc: 0.8360\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.4008 - acc: 0.8334\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.4012 - acc: 0.8350\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.4012 - acc: 0.8327\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.4006 - acc: 0.8352\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4005 - acc: 0.8329\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.4011 - acc: 0.8337\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.4006 - acc: 0.8351\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4004 - acc: 0.8335\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4002 - acc: 0.8347\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.4007 - acc: 0.8355\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.4006 - acc: 0.8337\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4004 - acc: 0.8335\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.4005 - acc: 0.8347\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4004 - acc: 0.8339\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.4004 - acc: 0.8351\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4002 - acc: 0.8342\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4007 - acc: 0.8344\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4005 - acc: 0.8344\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.4005 - acc: 0.8357\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.4003 - acc: 0.8347\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4005 - acc: 0.8354\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3999 - acc: 0.8350\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.4004 - acc: 0.8360\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4004 - acc: 0.8346\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.4001 - acc: 0.8354\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4008 - acc: 0.8354\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3997 - acc: 0.8346\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4002 - acc: 0.8356\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4003 - acc: 0.8347\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.4004 - acc: 0.8337\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4004 - acc: 0.8355\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.4003 - acc: 0.8346\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.4000 - acc: 0.8346\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4002 - acc: 0.8344\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.4003 - acc: 0.8340\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.4001 - acc: 0.8344\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.4004 - acc: 0.8351\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.4005 - acc: 0.8351\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.4002 - acc: 0.8347\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.4000 - acc: 0.8349\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.4001 - acc: 0.8340\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3999 - acc: 0.8362\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.3996 - acc: 0.8340\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.4001 - acc: 0.8344\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.4000 - acc: 0.8356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15503c50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ...\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)#if y_pred is larger than 0.5 it returns true(1) else false(2)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1559   36]\n",
      " [ 286  119]]\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839\n"
     ]
    }
   ],
   "source": [
    "accuracy=(1559+119)/2000 #Obtained from Confusion Matrix\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
